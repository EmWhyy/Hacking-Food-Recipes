{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune FLAN-T5 for chat & dialogue summarization\n",
    "\n",
    "In this blog, you will learn how to fine-tune [google/flan-t5-xl](https://huggingface.co/google/flan-t5-xl) for chat & dialogue summarization using Hugging Face Transformers. If you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages. \n",
    "\n",
    "In this example we will use the [samsum](https://huggingface.co/datasets/samsum) dataset a collection of about 16k messenger-like conversations with summaries. Conversations were created and written down by linguists fluent in English.\n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "1. [Setup Development Environment](#1-setup-development-environment)\n",
    "2. [Load and prepare samsum dataset](#2-load-and-prepare-samsum-dataset)\n",
    "3. [Fine-tune and evaluate FLAN-T5](#3-fine-tune-and-evaluate-flan-t5)\n",
    "4. [Run Inference and summarize ChatGPT dialogues](#4-run-inference-and-summarize-chatgpt-dialogues)\n",
    "\n",
    "Before we can start, make sure you have a [Hugging Face Account](https://huggingface.co/join) to save artifacts and experiments. \n",
    "\n",
    "## Quick intro: FLAN-T5, just a better T5\n",
    "\n",
    "FLAN-T5 released with the [Scaling Instruction-Finetuned Language Models](https://arxiv.org/pdf/2210.11416.pdf) paper is an enhanced version of T5 that has been finetuned in a mixture of tasks. The paper explores instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. The paper discovers that overall instruction finetuning is a general method for improving the performance and usability of pretrained language models. \n",
    "\n",
    "![flan-t5](../assets/flan-t5.png)\n",
    "\n",
    "* Paper: https://arxiv.org/abs/2210.11416\n",
    "* Official repo: https://github.com/google-research/t5x\n",
    "\n",
    "--- \n",
    "\n",
    "Now we know what FLAN-T5 is, let's get started. ðŸš€\n",
    "\n",
    "_Note: This tutorial was created and run on a g4dn.xlarge AWS EC2 Instance including a NVIDIA T4._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Development Environment\n",
    "\n",
    "Our first step is to install the Hugging Face Libraries, including transformers and datasets. Running the following cell will install all the required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytesseract in c:\\users\\florentin\\appdata\\roaming\\python\\python311\\site-packages (0.3.10)\n",
      "Requirement already satisfied: scikit-learn in f:\\conda2\\envs\\teamproject\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\florentin\\appdata\\roaming\\python\\python311\\site-packages (2.20.0)\n",
      "Requirement already satisfied: rouge-score in c:\\users\\florentin\\appdata\\roaming\\python\\python311\\site-packages (0.1.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\florentin\\appdata\\roaming\\python\\python311\\site-packages (3.8.1)\n",
      "Requirement already satisfied: tensorboard in f:\\conda2\\envs\\teamproject\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: py7zr in f:\\conda2\\envs\\teamproject\\lib\\site-packages (0.21.1)\n",
      "Requirement already satisfied: transformers[torch] in f:\\conda2\\envs\\teamproject\\lib\\site-packages (4.42.3)\n",
      "Requirement already satisfied: packaging>=21.3 in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from pytesseract) (23.2)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from pytesseract) (10.2.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from scikit-learn) (1.14.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\florentin\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: filelock in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\florentin\\appdata\\roaming\\python\\python311\\site-packages (from transformers[torch]) (0.23.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\florentin\\appdata\\roaming\\python\\python311\\site-packages (from transformers[torch]) (2024.5.15)\n",
      "Requirement already satisfied: requests in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from transformers[torch]) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\florentin\\appdata\\roaming\\python\\python311\\site-packages (from transformers[torch]) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\florentin\\appdata\\roaming\\python\\python311\\site-packages (from transformers[torch]) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\florentin\\appdata\\roaming\\python\\python311\\site-packages (from transformers[torch]) (4.66.4)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from transformers[torch]) (0.31.0)\n",
      "Requirement already satisfied: torch in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from transformers[torch]) (2.3.1+cu121)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\florentin\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\florentin\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\florentin\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\florentin\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\florentin\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\florentin\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\florentin\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: absl-py in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: six>=1.14.0 in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\florentin\\appdata\\roaming\\python\\python311\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from tensorboard) (1.64.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from tensorboard) (3.6)\n",
      "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from tensorboard) (4.25.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from tensorboard) (69.5.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from tensorboard) (3.0.3)\n",
      "Requirement already satisfied: texttable in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from py7zr) (1.7.0)\n",
      "Requirement already satisfied: pycryptodomex>=3.16.0 in c:\\users\\florentin\\appdata\\roaming\\python\\python311\\site-packages (from py7zr) (3.20.0)\n",
      "Requirement already satisfied: pyzstd>=0.15.9 in c:\\users\\florentin\\appdata\\roaming\\python\\python311\\site-packages (from py7zr) (0.16.0)\n",
      "Requirement already satisfied: pyppmd<1.2.0,>=1.1.0 in c:\\users\\florentin\\appdata\\roaming\\python\\python311\\site-packages (from py7zr) (1.1.0)\n",
      "Requirement already satisfied: pybcj<1.1.0,>=1.0.0 in c:\\users\\florentin\\appdata\\roaming\\python\\python311\\site-packages (from py7zr) (1.0.2)\n",
      "Requirement already satisfied: multivolumefile>=0.2.3 in c:\\users\\florentin\\appdata\\roaming\\python\\python311\\site-packages (from py7zr) (0.2.3)\n",
      "Requirement already satisfied: inflate64<1.1.0,>=1.0.0 in c:\\users\\florentin\\appdata\\roaming\\python\\python311\\site-packages (from py7zr) (1.0.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in c:\\users\\florentin\\appdata\\roaming\\python\\python311\\site-packages (from py7zr) (1.1.0)\n",
      "Requirement already satisfied: psutil in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from py7zr) (5.9.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\florentin\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\florentin\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\florentin\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\florentin\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from requests->transformers[torch]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from requests->transformers[torch]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from requests->transformers[torch]) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from requests->transformers[torch]) (2024.6.2)\n",
      "Requirement already satisfied: sympy in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from torch->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from torch->transformers[torch]) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from torch->transformers[torch]) (3.1.4)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from torch->transformers[torch]) (2021.4.0)\n",
      "Requirement already satisfied: colorama in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\florentin\\appdata\\roaming\\python\\python311\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: intel-openmp==2021.* in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->transformers[torch]) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->transformers[torch]) (2021.11.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in f:\\conda2\\envs\\teamproject\\lib\\site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# python\n",
    "!pip install pytesseract scikit-learn transformers[torch] datasets rouge-score nltk tensorboard py7zr --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: git-lfs in f:\\conda2\\envs\\teamproject\\lib\\site-packages (1.6)\n"
     ]
    }
   ],
   "source": [
    "# install git-fls for pushing model and logs to the hugging face hub\n",
    "!pip install git-lfs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example will use the [Hugging Face Hub](https://huggingface.co/models) as a remote model versioning service. To be able to push our model to the Hub, you need to register on the [Hugging Face](https://huggingface.co/join). \n",
    "If you already have an account, you can skip this step. \n",
    "After you have an account, we will use the `notebook_login` util from the `huggingface_hub` package to log into our account and store our token (access key) on the disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad015045cfb741968f3c78c8ea102d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare samsum dataset\n",
    "\n",
    "we will use the [samsum](https://huggingface.co/datasets/samsum) dataset a collection of about 16k messenger-like conversations with summaries. Conversations were created and written down by linguists fluent in English.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"13818513\",\n",
    "  \"summary\": \"Amanda baked cookies and will bring Jerry some tomorrow.\",\n",
    "  \"dialogue\": \"Amanda: I baked cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_id = \"samsum\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the `samsum` dataset, we use the `load_dataset()` method from the ðŸ¤— Datasets library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from the hub\n",
    "# dataset_old = datasets.load_dataset(dataset_id)\n",
    "\n",
    "# print(f\"Train dataset size: {len(dataset_old['train'])}\")\n",
    "# print(f\"Test dataset size: {len(dataset_old['test'])}\")\n",
    "\n",
    "# Train dataset size: 14732\n",
    "# Test dataset size: 819"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"/quaso data/dataset/transformed_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4462284\n",
      "4462280\n"
     ]
    }
   ],
   "source": [
    "print(data.size)\n",
    "data.dropna(inplace=True)\n",
    "print(data.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2170000.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20000.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#n = 2100000  #numbers under 2000000 will work\n",
    "#small did 181140\n",
    "#base did 51140\n",
    "m=61140\n",
    "data.drop(data.head(m).index, inplace=True)\n",
    "print(data.size / 2)\n",
    "\n",
    "n = 2150000\n",
    "data.drop(data.tail(n).index, inplace=True)\n",
    "(data.size / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61140    Title: All Bran Rolls Ingredients: shortening,...\n",
      "61141    Title: Batter-Fried Mushrooms Ingredients: mus...\n",
      "61142    Title: Triple Layer Pie Ingredients: instant c...\n",
      "Name: Input, dtype: object\n",
      "61140    Mix stir and cool shortening water sugar All B...\n",
      "61141    Wipe mushrooms with damp cloth. In a medium bo...\n",
      "61142    Prepare chocolate pudding with 1 3/4 cups milk...\n",
      "Name: Output, dtype: object\n"
     ]
    }
   ],
   "source": [
    "X = data[\"Input\"]\n",
    "y = data[\"Output\"]\n",
    "\n",
    "print(X.head(3))\n",
    "print(y.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18305 600 1095\n",
      "18305 600 1095\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.03)                     #20% test data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.0564)        #25% val data\n",
    "\n",
    "print(X_train.size, X_test.size, X_val.size)\n",
    "print(y_train.size, y_test.size, y_val.size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets checkout an example of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'target', '__index_level_0__'],\n",
      "        num_rows: 18305\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input', 'target', '__index_level_0__'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['input', 'target', '__index_level_0__'],\n",
      "        num_rows: 1095\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#print(dataset_old)\n",
    "dataset_train = datasets.Dataset.from_pandas(pd.DataFrame({\"input\": X_train, \"target\": y_train}))\n",
    "dataset_test = datasets.Dataset.from_pandas(pd.DataFrame({\"input\": X_test , \"target\": y_test }))\n",
    "dataset_val = datasets.Dataset.from_pandas(pd.DataFrame({\"input\": X_val, \"target\": y_val}))\n",
    "dataset = datasets.DatasetDict({\"train\": dataset_train,\n",
    "                                \"test\": dataset_test,\n",
    "                                \"val\": dataset_val})\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      "Title: Beulah'S Fruit Cake Ingredients: cake flour, margarine, sugar, egg yolks, soda, water, lemon extract, dates, white raisins, pecans, candied red cherries, candied green cherries, candied pineapple, egg whites\n",
      "---------------\n",
      "target: \n",
      "Cream well together margarine and sugar. Add beaten egg yolks. Add soda which has been dissolved in the water. Add lemon extract and mix thoroughly. Set aside. In a large bowl mix dates raisins pecans and candied fruits with about 1/4 of the flour. Mix remainder of flour into the sugar butter mixture then add fruit and nut mix and stir until thoroughly blended. Beat egg whites and fold in. Spray a large tube pan with nonstick spray. Pour in batter making sure it settles down into pan. Bake at 275\\u00b0 for 3 hours until wooden pick comes out clean.\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "from random import randrange        \n",
    "\n",
    "\n",
    "sample = dataset['train'][randrange(len(dataset[\"train\"]))]\n",
    "print(f\"input: \\n{sample['input']}\\n---------------\")\n",
    "print(f\"target: \\n{sample['target']}\\n---------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model we need to convert our inputs (text) to token IDs. This is done by a ðŸ¤— Transformers Tokenizer. If you are not sure what this means check out [chapter 6](https://huggingface.co/course/chapter6/1?fw=tf) of the Hugging Face Course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id=\"MettBrot/flan-t5-base-quaso-gen2.2\"\n",
    "\n",
    "# Load tokenizer of FLAN-t5-base\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before we can start training we need to preprocess our data. Abstractive Summarization is a text2text-generation task. This means our model will take a text as input and generate a summary as output. For this we want to understand how long our input and output will be to be able to efficiently batch our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17724e5d2c5a412b87a78c775399e28f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18905 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975dd51271744e87b10ead5012a2bd36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18905 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max target length: 319\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# The maximum total input sequence length after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"input\"], truncation=True), batched=True, remove_columns=[\"input\", \"target\"])\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"target\"], truncation=True), batched=True, remove_columns=[\"input\", \"target\"])\n",
    "max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n",
    "print(f\"Max target length: {max_target_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ae5a25fb304adebf79e8b08066e865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18305 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0937d8cd090c42048c597cacea303dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "814c8dd03b714ab89bc8101dfb33af2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1095 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(sample,padding=\"max_length\"):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [\"Create a recipe for: \" + item for item in sample[\"input\"]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample[\"target\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"input\", \"target\", \"__index_level_0__\"])\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tune and evaluate FLAN-T5\n",
    "\n",
    "After we have processed our dataset, we can start training our model. Therefore we first need to load our [FLAN-T5](https://huggingface.co/models?search=flan-t5) from the Hugging Face Hub. In the example we are using a instance with a NVIDIA V100 meaning that we will fine-tune the `base` version of the model. \n",
    "_I plan to do a follow-up post on how to fine-tune the `xxl` version of the model using Deepspeed._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# huggingface hub model id\n",
    "#model_id=\"google/flan-t5-small\"\n",
    "\n",
    "# load model from the hub\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, max_length=512)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to evaluate our model during training. The `Trainer` supports evaluation during training by providing a `compute_metrics`.  \n",
    "The most commonly used metrics to evaluate summarization task is [rogue_score](https://en.wikipedia.org/wiki/ROUGE_(metric)) short for Recall-Oriented Understudy for Gisting Evaluation). This metric does not behave like the standard accuracy: it will compare a generated summary against a set of reference summaries\n",
    "\n",
    "We are going to use `evaluate` library to evaluate the `rogue` score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\florentin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Metric\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# helper function to postprocess text\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4)/100 for k, v in result.items()}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start training is to create a `DataCollator` that will take care of padding our inputs and labels. We will use the `DataCollatorForSeq2Seq` from the ðŸ¤— Transformers library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to define the hyperparameters (`TrainingArguments`) we want to use for our training. We are leveraging the [Hugging Face Hub](https://huggingface.co/models) integration of the `Trainer` to automatically push our checkpoints, logs and metrics during training into a repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From f:\\conda2\\envs\\TeamProject\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# Hugging Face repository id\n",
    "repository_id = \"MettBrot/flan-t5-base-quaso-gen2.3\"\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=repository_id,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False, # Overflows with fp16\n",
    "    bf16=True,\n",
    "    # idk what the other guy wanted to tell me here but you cant use fp16 with this model you have to use bf16\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=5, #original 5\n",
    "    # logging & evaluation strategies\n",
    "    logging_dir=f\"{repository_id}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    # metric_for_best_model=\"overall_f1\",\n",
    "    # push to hub parameters\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub=False,\n",
    "    hub_strategy=\"every_save\",\n",
    "    hub_model_id=repository_id,\n",
    "    hub_token=HfFolder.get_token(),\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start our training by using the `train` method of the `Trainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be1adc8d3857482db99e34c5b6443ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22885 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8482, 'grad_norm': 1.9129873514175415, 'learning_rate': 4.890758138518681e-05, 'epoch': 0.11}\n",
      "{'loss': 1.838, 'grad_norm': 1.9941673278808594, 'learning_rate': 4.781516277037361e-05, 'epoch': 0.22}\n",
      "{'loss': 1.8428, 'grad_norm': 1.6862812042236328, 'learning_rate': 4.672274415556041e-05, 'epoch': 0.33}\n",
      "{'loss': 1.853, 'grad_norm': 2.1075921058654785, 'learning_rate': 4.5630325540747213e-05, 'epoch': 0.44}\n",
      "{'loss': 1.8367, 'grad_norm': 2.2247538566589355, 'learning_rate': 4.453790692593402e-05, 'epoch': 0.55}\n",
      "{'loss': 1.8282, 'grad_norm': 2.233309030532837, 'learning_rate': 4.344548831112082e-05, 'epoch': 0.66}\n",
      "{'loss': 1.8157, 'grad_norm': 1.971685528755188, 'learning_rate': 4.235306969630763e-05, 'epoch': 0.76}\n",
      "{'loss': 1.8068, 'grad_norm': 1.4842884540557861, 'learning_rate': 4.126065108149443e-05, 'epoch': 0.87}\n",
      "{'loss': 1.8222, 'grad_norm': 2.435084104537964, 'learning_rate': 4.016823246668123e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f1c601f7a94e099d275a0de2cd5567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.715450406074524, 'eval_rouge1': 0.356378, 'eval_rouge2': 0.131576, 'eval_rougeL': 0.28763, 'eval_rougeLsum': 0.33907699999999996, 'eval_gen_len': 36.44833333333333, 'eval_runtime': 162.2386, 'eval_samples_per_second': 3.698, 'eval_steps_per_second': 0.925, 'epoch': 1.0}\n",
      "{'loss': 1.7688, 'grad_norm': 1.8010356426239014, 'learning_rate': 3.907581385186804e-05, 'epoch': 1.09}\n",
      "{'loss': 1.763, 'grad_norm': 1.5233500003814697, 'learning_rate': 3.798339523705484e-05, 'epoch': 1.2}\n",
      "{'loss': 1.7776, 'grad_norm': 2.349514961242676, 'learning_rate': 3.689097662224165e-05, 'epoch': 1.31}\n",
      "{'loss': 1.7759, 'grad_norm': 2.3652114868164062, 'learning_rate': 3.579855800742845e-05, 'epoch': 1.42}\n",
      "{'loss': 1.7833, 'grad_norm': 1.68069589138031, 'learning_rate': 3.470613939261525e-05, 'epoch': 1.53}\n",
      "{'loss': 1.7518, 'grad_norm': 1.7118258476257324, 'learning_rate': 3.361372077780205e-05, 'epoch': 1.64}\n",
      "{'loss': 1.7677, 'grad_norm': 2.2422821521759033, 'learning_rate': 3.252130216298886e-05, 'epoch': 1.75}\n",
      "{'loss': 1.7725, 'grad_norm': 1.6141777038574219, 'learning_rate': 3.142888354817566e-05, 'epoch': 1.86}\n",
      "{'loss': 1.7824, 'grad_norm': 2.0355403423309326, 'learning_rate': 3.033646493336247e-05, 'epoch': 1.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f5cbfcd49bd4bf599d64c8463e4e2e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6988353729248047, 'eval_rouge1': 0.36676499999999995, 'eval_rouge2': 0.138924, 'eval_rougeL': 0.29584299999999997, 'eval_rougeLsum': 0.347785, 'eval_gen_len': 43.85166666666667, 'eval_runtime': 213.5583, 'eval_samples_per_second': 2.81, 'eval_steps_per_second': 0.702, 'epoch': 2.0}\n",
      "{'loss': 1.715, 'grad_norm': 2.2209997177124023, 'learning_rate': 2.9244046318549267e-05, 'epoch': 2.08}\n",
      "{'loss': 1.7016, 'grad_norm': 1.900525450706482, 'learning_rate': 2.815162770373607e-05, 'epoch': 2.18}\n",
      "{'loss': 1.7352, 'grad_norm': 2.150651454925537, 'learning_rate': 2.7059209088922875e-05, 'epoch': 2.29}\n",
      "{'loss': 1.7324, 'grad_norm': 1.8237974643707275, 'learning_rate': 2.596679047410968e-05, 'epoch': 2.4}\n",
      "{'loss': 1.7219, 'grad_norm': 2.2832772731781006, 'learning_rate': 2.4874371859296484e-05, 'epoch': 2.51}\n",
      "{'loss': 1.7214, 'grad_norm': 1.503172516822815, 'learning_rate': 2.378195324448329e-05, 'epoch': 2.62}\n",
      "{'loss': 1.7226, 'grad_norm': 2.2559854984283447, 'learning_rate': 2.268953462967009e-05, 'epoch': 2.73}\n",
      "{'loss': 1.7026, 'grad_norm': 1.8729499578475952, 'learning_rate': 2.1597116014856894e-05, 'epoch': 2.84}\n",
      "{'loss': 1.7382, 'grad_norm': 1.8101208209991455, 'learning_rate': 2.05046974000437e-05, 'epoch': 2.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5f8d5a62674118a7d45606751991d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6921210289001465, 'eval_rouge1': 0.367031, 'eval_rouge2': 0.138945, 'eval_rougeL': 0.297269, 'eval_rougeLsum': 0.34847700000000004, 'eval_gen_len': 40.32833333333333, 'eval_runtime': 187.0917, 'eval_samples_per_second': 3.207, 'eval_steps_per_second': 0.802, 'epoch': 3.0}\n",
      "{'loss': 1.6948, 'grad_norm': 1.8149487972259521, 'learning_rate': 1.94122787852305e-05, 'epoch': 3.06}\n",
      "{'loss': 1.7069, 'grad_norm': 2.060004949569702, 'learning_rate': 1.8319860170417304e-05, 'epoch': 3.17}\n",
      "{'loss': 1.6928, 'grad_norm': 2.342198371887207, 'learning_rate': 1.722744155560411e-05, 'epoch': 3.28}\n",
      "{'loss': 1.6813, 'grad_norm': 2.7239925861358643, 'learning_rate': 1.6135022940790913e-05, 'epoch': 3.39}\n",
      "{'loss': 1.6854, 'grad_norm': 2.1783459186553955, 'learning_rate': 1.5042604325977716e-05, 'epoch': 3.5}\n",
      "{'loss': 1.6814, 'grad_norm': 1.7549923658370972, 'learning_rate': 1.3950185711164519e-05, 'epoch': 3.6}\n",
      "{'loss': 1.676, 'grad_norm': 2.3355705738067627, 'learning_rate': 1.2857767096351323e-05, 'epoch': 3.71}\n",
      "{'loss': 1.6872, 'grad_norm': 2.058349847793579, 'learning_rate': 1.1765348481538126e-05, 'epoch': 3.82}\n",
      "{'loss': 1.7036, 'grad_norm': 2.124270439147949, 'learning_rate': 1.0672929866724929e-05, 'epoch': 3.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd38b7d6b584716a3f73f1ac7a7efb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.688697099685669, 'eval_rouge1': 0.37326, 'eval_rouge2': 0.142172, 'eval_rougeL': 0.300908, 'eval_rougeLsum': 0.353696, 'eval_gen_len': 42.035, 'eval_runtime': 201.231, 'eval_samples_per_second': 2.982, 'eval_steps_per_second': 0.745, 'epoch': 4.0}\n",
      "{'loss': 1.6708, 'grad_norm': 1.838274359703064, 'learning_rate': 9.580511251911733e-06, 'epoch': 4.04}\n",
      "{'loss': 1.6628, 'grad_norm': 1.4072484970092773, 'learning_rate': 8.488092637098537e-06, 'epoch': 4.15}\n",
      "{'loss': 1.6523, 'grad_norm': 2.729693651199341, 'learning_rate': 7.39567402228534e-06, 'epoch': 4.26}\n",
      "{'loss': 1.6546, 'grad_norm': 2.911719560623169, 'learning_rate': 6.303255407472143e-06, 'epoch': 4.37}\n",
      "{'loss': 1.6567, 'grad_norm': 1.6340956687927246, 'learning_rate': 5.210836792658947e-06, 'epoch': 4.48}\n",
      "{'loss': 1.686, 'grad_norm': 2.389305591583252, 'learning_rate': 4.118418177845751e-06, 'epoch': 4.59}\n",
      "{'loss': 1.6728, 'grad_norm': 2.219902992248535, 'learning_rate': 3.025999563032554e-06, 'epoch': 4.7}\n",
      "{'loss': 1.6738, 'grad_norm': 1.8918577432632446, 'learning_rate': 1.9335809482193577e-06, 'epoch': 4.81}\n",
      "{'loss': 1.6779, 'grad_norm': 1.8393601179122925, 'learning_rate': 8.411623334061612e-07, 'epoch': 4.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad1a248774e4f60b9d8d94acb435da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6873698234558105, 'eval_rouge1': 0.371005, 'eval_rouge2': 0.142546, 'eval_rougeL': 0.300257, 'eval_rougeLsum': 0.35225, 'eval_gen_len': 43.495, 'eval_runtime': 216.3902, 'eval_samples_per_second': 2.773, 'eval_steps_per_second': 0.693, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 7910.8687, 'train_samples_per_second': 11.57, 'train_steps_per_second': 2.893, 'train_loss': 1.735341051704685, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=22885, training_loss=1.735341051704685, metrics={'train_runtime': 7910.8687, 'train_samples_per_second': 11.57, 'train_steps_per_second': 2.893, 'total_flos': 1.27303346386944e+16, 'train_loss': 1.735341051704685, 'epoch': 5.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![flan-t5-tensorboard](../assets/flan-t5-tensorboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, we have trained our model. ðŸŽ‰ Lets run evaluate the best model again on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b0e8b948a4c4cc5958ed0ca526c7c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.6873698234558105,\n",
       " 'eval_rouge1': 0.371005,\n",
       " 'eval_rouge2': 0.142546,\n",
       " 'eval_rougeL': 0.300257,\n",
       " 'eval_rougeLsum': 0.35225,\n",
       " 'eval_gen_len': 43.495,\n",
       " 'eval_runtime': 214.3138,\n",
       " 'eval_samples_per_second': 2.8,\n",
       " 'eval_steps_per_second': 0.7,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score we achieved is an `rouge1` score of `47.23`. \n",
    "\n",
    "Lets save our results and tokenizer to the Hugging Face Hub and create a model card. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab54e54cd2245659b3b700f38d7e575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1720013070.Florentin-PC.26736.0:   0%|          | 0.00/18.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ad64c9078e94af996a60221946fff54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b9a7be86cb495e8307a27dbcc5f80d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7109fc2af11143e29983ccce4c92784c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08bd063bc6364ccd8d94bedcfe829720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1720021195.Florentin-PC.26736.1:   0%|          | 0.00/623 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/MettBrot/flan-t5-base-quaso-gen2.3/commit/2d7baf56494042b738aebe40e58d5ed4cbd0fab4', commit_message='End of training', commit_description='', oid='2d7baf56494042b738aebe40e58d5ed4cbd0fab4', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save our tokenizer and create model card\n",
    "tokenizer.save_pretrained(repository_id)\n",
    "trainer.create_model_card()\n",
    "# Push the results to the hub\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Inference\n",
    "\n",
    "Now we have a trained model, we can use it to run inference. We will use the `pipeline` API from transformers and a `test` example from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      "Create a recipe for: Title: Krispy Cheese Wafers Ingredients: oleo, flour, Cheddar cheese, Rice Krispies, red pepper\n",
      "---------------\n",
      "Target: \n",
      "Mix all ingredients thoroughly. Shape into balls. Place on a greased cookie sheet. Flatten with bottom of a glass dipped in flour. Bake at 350\\u00b0 until done.\n",
      "---------------\n",
      "quaso small gen 3 summary:\n",
      "[{'generated_text': 'Mix all ingredients together. Roll into small balls. Bake at 350u00b0 for 10 to 12 minutes.'}]\n",
      "---------------\n",
      "quaso base gen 2.2 summary:\n",
      "[{'generated_text': 'Melt oleo in a 9 x 13-inch pan. Mix flour and cheese. Add Rice Krispies and red pepper. Mix well. Spread on oleo. Bake at 350u00b0 for 15 minutes.'}]\n",
      "quaso base gen 2.1 summary:\n",
      "[{'generated_text': 'Melt oleo in a large skillet. Add flour and stir until smooth. Add cheese and stir until melted. Add Rice Krispies and stir until well blended. Drop by teaspoonfuls onto ungreased cookie sheet. Bake at 350u00b0 for 10 to 12 minutes.'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from random import randrange        \n",
    "\n",
    "model_id = \"MettBrot/flan-t5-small-quaso-gen3\"\n",
    "\n",
    "# load model and tokenizer from huggingface hub with pipeline\n",
    "summarizer = pipeline(\"text2text-generation\", model=model_id, device=0, max_length=1000)\n",
    "\n",
    "model_id_old = \"MettBrot/flan-t5-base-quaso-gen2.3\"\n",
    "\n",
    "summarizer_old = pipeline(\"text2text-generation\", model=model_id_old, device=0, max_length=1000)\n",
    "\n",
    "model_id_older = \"MettBrot/flan-t5-base-quaso-gen2.2\"\n",
    "\n",
    "summarizer_older = pipeline(\"text2text-generation\", model=model_id_older, device=0, max_length=1000)\n",
    "\n",
    "# select a random test sample\n",
    "sample = dataset['test'][randrange(len(dataset[\"test\"]))]\n",
    "print(f\"Input: \\nCreate a recipe for: {sample['input']}\\n---------------\")\n",
    "\n",
    "print(f\"Target: \\n{sample['target']}\\n---------------\")\n",
    "\n",
    "# summarize dialogue\n",
    "res = summarizer(\"Create a recipe for: \" + sample[\"input\"])\n",
    "\n",
    "print(f\"quaso small gen 3 summary:\\n{res}\\n---------------\")\n",
    "\n",
    "res_old = summarizer_old(\"Create a recipe for: \" + sample[\"input\"])\n",
    "\n",
    "print(f\"quaso base gen 2.2 summary:\\n{res_old}\")\n",
    "\n",
    "res_older = summarizer_older(\"Create a recipe for: \" + sample[\"input\"])\n",
    "\n",
    "print(f\"quaso base gen 2.1 summary:\\n{res_older}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
